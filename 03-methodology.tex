\newcommand{\SAMPN}{100}
\newcommand{\SAMPK}{2 million}
\newcommand{\SAMP}{1.4 million}
\newcommand{\WARM}{100 thousand}
\newcommand{\COOL}{500 thousand}

\section{Methodology}
We simulate a set associative cache using a number of different
replacement policies.
We use a statistical sampling technique to keep our total simulation
time down while still ensuring our results are representative of the
overall behavior for the application in question.

We chose to use the NAS Parallel Benchmarks~\cite{bailey94} as a set of reference programs to explore our metrics.
We specifically ended up using sp\_omp with only one thread in this paper because it showed the most interesting results.
PIN~\cite{lukcohn05} was used to get a memory trace of the program,
 then this memory trace was sampled in a manner that will be discussed in \ref{sec:sampling},
 and then the sampled traces were run through a Cache policy simulator, which we wrote in Python.
Using the cache replacement results and the original sampled traces, we then constructed MATR, MMTR, and WTTR for each policy.

\subsection{Sampling Technique}
\label{sec:sampling}
We sampled the file to minimize the data set we operated on, while maintaining the integrity with results.
	We took \SAMPN~samples of \SAMPK~memory accesses generated by sp\_omp instrumented by PIN.
	By the central limit theorem we can approximate samples of this size as having a Gaussian distribution.
	Each sample was divided into 3 sections, a warm-up period of \WARM~memory accesses,
		a sampling period of \SAMP~memory accesses, and a cool-down period of \COOL~memory accesses.
	For each metric we warmed up the cache during the warm-up period,
		then measured how long it took for any cache line evicted during the sampling period to be recached.
	Cache lines that took longer than the cool-down period to recache were ignored.
	We felt that cache lines that took more than \COOL~memory accesses to recache were unlikely to be interesting.

\subsection{Caching Policies}
\label{sec:policies}

Accessing a cache line for the first time causes it to be added to the cache.
Each cache line contains 64 bytes of data.
We use 7 of the cache line address bits to decide which set the cache line maps to.
Each set is a 8-way associative set, and when all of the ways in a set are filled,
 and we want to insert another cache line into the set, then one of the old cache lines must be selected for eviction to make way for new data.
We test several different cache replacement policies which determine the cache line to be evicted in the event of an associativity conflict.
The new cache line is always placed in the same cache way that was vacated by the evicted data.

\subsubsection{Belady}

Belady's algorithm~\cite{belady66} was proposed in the context of page replacement, but its principle also applies in cache replacement.  The algorithm is to choose the cache line to be evicted whose next access is furthest in the future.  This is the optimal cache replacement strategy, but it requires oracle knowledge of the total order of all memory accesses in the program, which is not generally knowable at the time of cache replacement.  We show TTR results for Belady's algorithm anyway as an optimal baseline to compare the other cache replacement algorithms against.

\subsubsection{Random}

While Belady's algorithm requires perfect knowledge of all memory accesses in a program, the Random cache replacement algorithm requires no knowledge whatsoever about the order of memory accesses in the program.  When each cache replacement decision is to be made, a random number is generated that decides which way of the set to evict.

All of the other evaluated cache replacement policies will behave differently because of, and respond to, program behavior in one way or another, but the Random algorithm is not affected in any way by program behavior.  This algorithm requires the storage of no additional state per cache line to implement.

\subsubsection{FIFO}

First In First Out, or FIFO, chooses the cache line to evict from the set that has been in the cache longest.  Neither reuse, nor any other factors, has any bearing in deciding which cache line to evict in the FIFO algorithm.  This algorithm requires only enough per-set state to point to the cache line in that set that will be evicted and replaced next.
\subsubsection{LRU}

Least Recently Used, or LRU, choose to evict the cache line from the set whose most recent access was furthest in the past.  Conceptually this works like a queue, where eviction candidates are always chosen from the tail of the queue, and both initial use and subsequent reuse promotes cache lines to the front of the queue, furthest away from being in danger of eviction. In the absence of cache line reuse, this algorithm behaves identically to FIFO.  This algorithm requires enough state per cache line to enforce an absolute order between all members of a set, typically log$_2$(N) bits per cache line, where N is the associativity of the set.

%% \subsubsection{MRU}

%% Most Recently Used, or MRU, behaves in the opposite way of LRU.  The same state and ordering is being maintained in both algorithms, but in MRU the head of the queue, the most recently used cache line, is chosen for eviction, rather than the tail of the queue.  This algorithm has the same storage overheads as LRU.

\subsubsection{NRU}

Not Recently Used, or NRU, is a simplification of the mechanism and storage overheads in LRU, and is similar in operation to the clock algorithm in page replacement.  There is 1 bit of state maintained per cache line in a set.  A value of ``0'' means that the cache line has been ``recently used,'' and is therefore not a candidate for eviction.  A value of ``1'' means that the cache line has ``not been recently used,'' and may be evicted.  Re-referencing a cache line causes its NRU state to be set to ``0''.

The eviction victim is found by linearly scanning through the NRU state bits in a set (in a left-to-right fashion) and evicting the cache line corresponding to the first ``1'' encountered. 
If no ``1''s are encountered, then all NRU bits are changed to 1 and the same victim selection process is repeated, i.e., the cache line in way ``0'' will be evicted.

%% \subsubsection{NRU Random}

%% NRU Random is similar to NRU, but rather than a linear scan through all of the NRU bits to evict the first encountered 1, a random cache line is chosen whose NRU bit is set to 1.  Again, if there are no cache lines have an NRU bit value of 1, then all cache lines NRU bits are changed to 1.

\subsubsection{SRRIP}

Static Re-Reference Interval Prediction~\cite{jaleeltheobald10}, or SRRIP, is similar to NRU, but it uses multiple bits to encode how recently a cache line has been used, and therefore how soon it is predicted to be used again.  For this study, we use 2 bits to encode the Re-Reference Prediction Values (RRPVs) for all RRIP-derived cache replacement algorithms, meaning that RRPVs are in the range of 0 to 3.  An RRPV of 0 means that the cache line was recently used, and is in little danger of eviction, and an RRPV of 3 means that the cache line was not recently used, and is in danger of imminent eviction.

SRRIP works by assigning an RRPV of 2 to each cache line when it is initially brought into the cache.  Reuse of a cache line promotes its RRPV to 0.  Eviction victim selection is done by scanning through the set, left-to-right, and looking for a cache line whose RRPV is 3.  The first encountered 3 is evicted.  If no 3s are found, then all RRPVs in the set are incremented, and the scan to look for a 3 repeats.  This process may repeat several times until a 3 is found and evicted.

\subsubsection{BRRIP}

Bimodal RRIP~\cite{jaleeltheobald10}, or BRRIP is similar to SRRIP, and varies only in the initial RRPV of cache lines when they are first brought into the cache.  Whereas SRRIP always assigns an RRPV of 2 to incoming cache lines, BRRIP assigns an RRPV of 3 95\% of the time, and an RRPV of 2 only 5\% of the time.  This offers resistance to scans through memory that may otherwise thrash the cache.  In the absence of cache line reuse, BRRIP results in only a single way of each set being thrashed, rather than every way, as can happen in LRU, NRU, or SRRIP.

The mechanisms of scanning through the set to find an RRPV of 3, and incrementing all RRPVs if a 3 is not found, are identical to SRRIP.

\subsubsection{DRRIP}

Dynamic RRIP~\cite{jaleeltheobald10}, or DRRIP, chooses to use either SRRIP or BRRIP at run-time, depending on which cache replacement policy is currently more effective at minimizing cache misses.  This is done by permanently assigning a small number of sets in the cache to always follow the SRRIP policy, and permanently assigning a small number of other sets to always follow the BRRIP policy.  Each cache miss in a SRRIP-only set will increment a policy selection counter, and each cache miss in a BRRIP-only set will decrement the policy selection counter.

Depending on the value of the policy selection counter, either SRRIP or BRRIP will be considered to currently be causing more cache misses than the other.  The remaining cache sets that are not permanently assigned to follow a specific cache replacement policy will then use the cache replacement policy of whichever candidate policy is currently causing the fewest cache misses.

The mechanisms of scanning through the set to find an RRPV of 3, and incrementing all RRPVs if a 3 is not found, are identical to SRRIP and BRRIP for all sets, regardless of which replacement policy they are using.

